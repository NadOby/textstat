{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# coding: utf-8\n",
    "!pip install -r requrements.txt\n",
    "import nltk\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz, lil_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from collections import OrderedDict\n",
    "#np.set_printoptions(threshold=np.inf)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "extended_punctuation = sorted(set(punctuation).union(set(['–', '…', '«', '»', '*'])))\n",
    "\n",
    "#text = 'Хотя сам в первый раз минут тридцать трясся, все не мог понять, этот водоворот черный или же в нем есть три желтые песчинки, что мерещатся мне в который уже раз... уф...' *2\n",
    "#text = 'a b c b a a'\n",
    "corpusdir = os.path.expanduser(\"~\") + '/book/'\n",
    "files = os.listdir(corpusdir)\n",
    "text_dict = {}\n",
    "str_punkt = ''.join(extended_punctuation)\n",
    "str_empty = ' ' * len(str_punkt)\n",
    "\n",
    "def cleanup(txt):\n",
    "    table = str.maketrans(str_punkt, str_empty)\n",
    "    txt = txt.translate(table).lower()\n",
    "    table_2 = str.maketrans('qwertyuiopasdfghjklzxcvbnm0123456789', str_empty)\n",
    "    txt = txt.translate(table_2)\n",
    "    return txt\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    fullpath =  corpusdir + file\n",
    "    print(\"reading from {}\".format(fullpath))\n",
    "    text = open( fullpath, 'r').read()\n",
    "    text_dict[file] = cleanup(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word_dict = {}\n",
    "for k, v in text_dict.items():\n",
    "    word_dict[k] = [stemmer.stem(word) for word in nltk.word_tokenize(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for k, v in word_dict.items():\n",
    "    print(k)\n",
    "    print(' '.join(v[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#text.replace('-', ' ')\n",
    "#words = [stemmer.stem(word) for word in nltk.word_tokenize(text.replace('-', ' ').lower()) if word not in extended_punctuation]\n",
    "words = []\n",
    "for i in word_dict.values():\n",
    "    words = words + i\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lemlist = sorted([x for x in (set(words))])\n",
    "order = len(lemlist)\n",
    "print(len(words), order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#M = np.zeros((order,order))\n",
    "#M = csr_matrix((order, order))\n",
    "M = lil_matrix((order, order))\n",
    "print(type(M))\n",
    "print(M.shape)\n",
    "print(M[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for n, m in word_dict.items():\n",
    "    print(\"Working on \" + n)\n",
    "    for i, j in zip(m[:-1],m[1:]):\n",
    "        x = lemlist.index(i)\n",
    "        y = lemlist.index(j)\n",
    "        #print(i,j,x,y)\n",
    "        M[x, y] = M[x, y] + 1\n",
    "#print(\"  \" + \" \".join(words))\n",
    "#print(\"  \" + \" \".join(lemlist))\n",
    "#print(M.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "M_normalized = normalize(M, norm='l1', axis=1, copy=False)\n",
    "#M.todence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "save_npz('book_mm.npz', M_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "open('lemlist.txt', 'w').write('\\n'.join(lemlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
